{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import dotenv, os\n",
    "from pprint import pprint\n",
    "\n",
    "# Import and set up OCR pipeline\n",
    "\n",
    "dotenv.load_dotenv(\".env\", override=True)\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "\n",
    "print(\"hello, world!\")\n",
    "\n",
    "print('USE_TORCH = ', os.environ.get('USE_TORCH'))\n",
    "\n",
    "\n",
    "model = ocr_predictor(det_arch=\"linknet_resnet18\", reco_arch=\"crnn_mobilenet_v3_small\", assume_straight_pages=True, det_bs=2048, reco_bs=2048, pretrained=True)\n",
    "\n",
    "model.det_predictor.model.postprocessor.bin_thresh = 0.002\n",
    "model.det_predictor.model.postprocessor.box_thresh = 0.002\n",
    "\n",
    "\n",
    "print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR PIPELINE :)\n",
    "%%time\n",
    "def worker(file_name):\n",
    "    doc = DocumentFile.from_images(file_name)\n",
    "    with torch.no_grad():\n",
    "        res =  model(doc)\n",
    "        # checks for valid rotation\n",
    "        line = [block[\"lines\"][0][\"words\"] for block in res.export()[\"pages\"][0][\"blocks\"]]\n",
    "        confidences = [word[\"confidence\"] > .5 for block in line for word in block]\n",
    "        validity = sum(confidences) / len(confidences)\n",
    "        if validity < .85:\n",
    "            print(\"Upside down image detected.\")\n",
    "            rotated = (imutils.rotate(cv2.imread(file_name), angle=180))\n",
    "            cv2.imwrite(file_name, rotated)\n",
    "            doc = DocumentFile.from_images(file_name)\n",
    "            new_res = model(doc)\n",
    "            line = [block[\"lines\"][0][\"words\"] for block in new_res.export()[\"pages\"][0][\"blocks\"]]\n",
    "            confidences = [word[\"confidence\"] > .5 for block in line for word in block]\n",
    "            new_validity = sum(confidences) / len(confidences)\n",
    "            if new_validity < .85:\n",
    "                unreadable.append(file_name)\n",
    "            if new_validity > validity:\n",
    "                res = new_res\n",
    "                validity = new_validity\n",
    "        print(f\"{progress[file_name]}: {file_name} | {validity}\")\n",
    "        return res\n",
    "\n",
    "progress = {}\n",
    "tasks = []\n",
    "results = []\n",
    "unreadable = []\n",
    "folder = \"pdfs/tmp/\"\n",
    "for i, name in enumerate(os.listdir(folder)):\n",
    "    if \"jpeg\" in name:\n",
    "        file_path = \"pdfs/tmp/\" + name\n",
    "        tasks.append(file_path)\n",
    "        progress[file_path] = i\n",
    "\n",
    "print(\"setup complete\",end='\\n\\n')\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(worker)(x) for x in tasks)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "for result in results:\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "with open(\"ocr-data.json\", \"w\") as file:\n",
    "    for result in results:\n",
    "        json_output = result.export()\n",
    "        json.dump(json_output, file)\n",
    "        file.write('\\n\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match results from OCR to filenames based on OCR logs\n",
    "connections = []\n",
    "with open(\"order.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        line = line.rstrip()\n",
    "        if \"jpeg\" in line:\n",
    "            title = line.split(\":\")\n",
    "            res_title = title[1].split(\"|\")[0][27:-6]\n",
    "            connections.append([title[0], res_title])\n",
    "\n",
    "connections.sort(key=lambda x: int(x[0]))\n",
    "order = []\n",
    "for itm in connections:\n",
    "    order.append(itm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved data and add it back to results variable\n",
    "with open(\"ocr-data.json\", \"r\") as file:\n",
    "    all_data = file.read()\n",
    "    split = all_data.split(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print of OCR results\n",
    "\n",
    "# # Use this if not reading from saved data\n",
    "# for result in results:\n",
    "#     json_output = result.export()\n",
    "\n",
    "# Use this if reading from saved data\n",
    "for item in split:\n",
    "    # a new line must be causing an issue in dividing up the results\n",
    "    # somewhere in the json file.\n",
    "    try:\n",
    "        json_output = json.loads(item)\n",
    "    except:\n",
    "        break\n",
    "   \n",
    "    blocks = json_output[\"pages\"][0][\"blocks\"]\n",
    "    for block in blocks:\n",
    "        for line in block[\"lines\"]:\n",
    "            words = line[\"words\"]\n",
    "            print(\"-------\")\n",
    "            line = []\n",
    "            confidences = []\n",
    "            for word in words:\n",
    "                geo = word[\"geometry\"]\n",
    "                size = (geo[1][0] - geo[0][0]) * 100000 * (geo[1][1] - geo[0][1])\n",
    "                if size > 35:\n",
    "                    confidences.append(word[\"confidence\"])\n",
    "                    if word[\"confidence\"] >= 0.5:\n",
    "                        line.append(word[\"value\"])\n",
    "            if len(confidences) > 0:\n",
    "                if sum(confidences)/ len(confidences) > 0.7:\n",
    "                    print(\" \".join(line))\n",
    "                # else:\n",
    "                    # print(\"ERROR: \" + \" \".join(line))\n",
    "    print(\"\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates low confidence data spreadsheet\n",
    "file = open(\"output.csv\", \"w\")\n",
    "for i, result in enumerate(results):\n",
    "    json_output = result.export()\n",
    "    blocks = json_output[\"pages\"][0][\"blocks\"]\n",
    "    for block in blocks:\n",
    "        words = block[\"lines\"][0][\"words\"]\n",
    "        line = []\n",
    "        confidences = []\n",
    "        for word in words:\n",
    "            geo = word[\"geometry\"]\n",
    "            size = (geo[1][0] - geo[0][0]) * 100000 * (geo[1][1] - geo[0][1])\n",
    "\n",
    "            if word[\"confidence\"] < 0.5:\n",
    "                conf = word[\"confidence\"] * 100\n",
    "                file.write(\"{}\\t{:.2f}\\t{:.2f}\\t{}\\n\".format(word[\"value\"], size, conf))\n",
    "file.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse whitestudiocards to get title, desc, retrieval, etc.\n",
    "def num_freq(searchee):\n",
    "    count = 0\n",
    "    for char in searchee:\n",
    "        if char.isnumeric():\n",
    "            count += 1\n",
    "    return count / len(searchee)\n",
    "\n",
    "cards = {}\n",
    "# toggle depending on reading from saved data\n",
    "toggle = split\n",
    "# toggle = results\n",
    "for card_num, result in enumerate(toggle):\n",
    "    try:\n",
    "        name = order[card_num]\n",
    "        new_card = {\"desc_list\": [], \"date\": \"\", \"year\": []}\n",
    "        cards[name] = new_card\n",
    "        # toggle depending on reading from saved data\n",
    "        # json_output = result.export()\n",
    "        json_output = json.loads(result)\n",
    "        blocks = json_output[\"pages\"][0][\"blocks\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, block in enumerate(blocks):\n",
    "        for line in block[\"lines\"]:\n",
    "            words = line[\"words\"]\n",
    "            # print(\"-------\")\n",
    "            line = []\n",
    "            confidences = []\n",
    "            for word in words:\n",
    "                geo = word[\"geometry\"]\n",
    "                size = (geo[1][0] - geo[0][0]) * 100000 * (geo[1][1] - geo[0][1])\n",
    "                if size > 35:\n",
    "                    confidences.append(word[\"confidence\"])\n",
    "                    if word[\"confidence\"] >= 0.5:\n",
    "                        line.append(word[\"value\"])\n",
    "            if len(confidences) > 0:\n",
    "                if sum(confidences)/ len(confidences) > 0.7:\n",
    "                    text = (\" \".join(line))\n",
    "                    if i == 0:\n",
    "                        new_card[\"job\"] = text\n",
    "                    if i == 1 or i == 2:\n",
    "                        flagged = False\n",
    "                        if num_freq(text) > .7 :\n",
    "                            new_card[\"retrieval\"] = text\n",
    "                            if i == 2:\n",
    "                                flagged = True\n",
    "                        else:\n",
    "                            if \"job\" not in new_card:\n",
    "                                new_card[\"job\"] = text\n",
    "                        if \"retrieve\" in text.lower() and \"retrieval\" not in new_card:\n",
    "                            # cut off \"retrieve : \"\n",
    "                            new_card[\"retrieval\"] = text[11:]\n",
    "\n",
    "                    if i >= 2:\n",
    "                        if \"retrieve\" not in text.lower() and not flagged:\n",
    "                            matches = re.findall(r\"(18\\d{2}|19\\d{2}|20\\d{2})\", text)\n",
    "                            if len(matches) > 0:\n",
    "                                new_card[\"date\"] = text\n",
    "                                for match in matches:\n",
    "                                    new_card[\"year\"].append(int(match))\n",
    "                            new_card[\"desc_list\"].append(text)\n",
    "                        flagged = False\n",
    "                else:\n",
    "                    # print(\"ERROR: \" + \" \".join(line))\n",
    "                    pass\n",
    "\n",
    "    for j in range(1, len(new_card[\"desc_list\"]), 3):\n",
    "        # line near last is mostly numbers, it's data that's not needed\n",
    "        try:\n",
    "            if num_freq(new_card[\"desc_list\"][-j].replace(\" \", \"\")) > .4:\n",
    "                del new_card[\"desc_list\"][-j]\n",
    "            else:  # we've hit actual text\n",
    "                break\n",
    "        except: # Index error\n",
    "            pass\n",
    "\n",
    "\n",
    "    new_card[\"description\"] = \" \".join(new_card[\"desc_list\"])\n",
    "    # print(\"File: \" + name)\n",
    "    # for key, value in new_card.items():\n",
    "    #     if key != \"desc_list\":\n",
    "    #         print(f\"{key.upper()}: {value}\")\n",
    "    # print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "\n",
    "\"\"\"\n",
    "In this example 40 titles are extracted, yet 2,238 results should be reachable.\n",
    "\"\"\"\n",
    "\n",
    "def trawler(uuid):\n",
    "    global count, headers\n",
    "    url = f\"https://api.repo.nypl.org/api/v2/collections/{uuid}\"\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    res = resp.json()\n",
    "    response = res[\"nyplAPI\"][\"response\"]\n",
    "\n",
    "    if \"collection\" in response:  # We've not hit item level yet\n",
    "        items = response[\"collection\"]\n",
    "         # Some items in the json are dicts and others are lists of dicts\n",
    "        if isinstance(items, dict):\n",
    "            items = [items]\n",
    "\n",
    "        # If still a parent item or a container, keep traversing down\n",
    "        for item in items:\n",
    "            if int(item[\"numSubCollections\"]) > 0:\n",
    "                trawler(item[\"uuid\"])\n",
    "            elif item[\"type\"] == \"Container\":\n",
    "                trawler(item[\"uuid\"])\n",
    "\n",
    "    else:\n",
    "        items = response[\"item\"]\n",
    "        if isinstance(items, dict):\n",
    "            items = [items]\n",
    "\n",
    "        for item in items:\n",
    "            titles = item[\"mods\"][\"titleInfo\"]\n",
    "            \n",
    "            if isinstance(titles, dict):\n",
    "                titles = [titles]\n",
    "    \n",
    "            for title in titles:\n",
    "                count += 1\n",
    "                print(title[\"title\"])\n",
    "   \n",
    "headers = CaseInsensitiveDict()\n",
    "token = \"\"\n",
    "with open(\"api_token.txt\", \"r\") as file:\n",
    "    token = file.read()\n",
    "\n",
    "headers[\"Authorization\"] = f\"Token token=\\\"{token}\\\"\"\n",
    "\n",
    "count = 0  # for keeping track of extracted titles\n",
    "\n",
    "# UUID of White Studio collection\n",
    "trawler(\"7c22cac0-c5b8-012f-4613-58d385a7bc34\")\n",
    "print(f\"Total extracted titles: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting digitzed White Studio cards together (just cleaning scrapped web data)\n",
    "titles = set()\n",
    "with open(\"input2.txt\") as file:\n",
    "    data = file.readlines()\n",
    "    for line in data:\n",
    "        line = line.rstrip()\n",
    "        if len(line) > 1:\n",
    "            if line[-1] == \".\":\n",
    "                line = line[:-1]\n",
    "            titles.add(line)\n",
    "with open(\"input.txt\") as file:\n",
    "    data = file.readlines()\n",
    "    for line in data:\n",
    "        line = line.rstrip()\n",
    "        if len(line) > 2:\n",
    "            titles.add(line)\n",
    "\n",
    "with open(\"whitestudio-digitized.txt\", \"w\") as file:\n",
    "    for title in titles:\n",
    "        file.write(f\"{title.rstrip()}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching White Studio cards to digital collections\n",
    "old_count = 0\n",
    "count = 0\n",
    "# Compiled from manual review\n",
    "mismatches = [4,7,12,21,24,26,27,35,36,37,38,39,41,42,43,48,49,50,57,60,68,73,74,80,84,85,86,87,88,89,93]\n",
    "print(\"\\t\\t\\t\\tCatalog Card + Date | Digital Collections | Filename\")\n",
    "for card in cards:\n",
    "    try:\n",
    "        job = cards[card][\"job\"].lower()\n",
    "        dates = cards[card][\"year\"]\n",
    "        if len(dates) > 0:\n",
    "            dates = \" \".join([str(x) for x in dates])\n",
    "        else:\n",
    "            dates = \"\"\n",
    "    except:\n",
    "        # these cards have edge cases... will come back to later\n",
    "        # print(card, cards[card])\n",
    "        pass\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        match_count = len(set(re.findall(r'\\b\\w+\\b', job)) & set(re.findall(r'\\b\\w+\\b', title)))\n",
    "        if match_count > 2:\n",
    "            old_count += 1\n",
    "            if old_count not in mismatches:\n",
    "                count +=1\n",
    "                print(f\"Potential match #{count} found:\\t{job} {dates} | {title} | {card}\")\n",
    "print(f\"Total potential matches: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Web scrape all white studios catalog\n",
    "\n",
    "def get_class(url, name, page):\n",
    "    test = None\n",
    "    while test == None:\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        html = resp.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        test = soup.find(\"dd\", attrs={\"data\": \"definition-Call Number\"})\n",
    "        if test == None:\n",
    "            print(f\"{name} retrying page {page}... url {url}\")\n",
    "            # edge case, no class mark\n",
    "            for case in edge_cases:\n",
    "                if case in name:\n",
    "                    return\n",
    "            time.sleep(2)\n",
    "    inner_span = test.find(\"span\").find(\"span\")\n",
    "    if inner_span:\n",
    "        call_number_text = inner_span.text.strip()\n",
    "        with open(\"catalog-scraping-final.csv\", \"a\") as file:\n",
    "            file.write(\"{}\\t{}\\t{}\\n\".format(page, name, call_number_text))\n",
    "        print(f\"Succsess page {page}\")\n",
    "    else:\n",
    "        print(f\"Page {page} error: {url}\")\n",
    "\n",
    "        \n",
    "edge_cases = [\"Esther Morris gets women the vote\", \"Tyler Perry's Boo\"]\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"}\n",
    "for i in range(30,31):\n",
    "    url = f\"https://www.nypl.org/research/research-catalog/search?q=White%20Studio&search_scope=contributor&page={i}\"\n",
    "    results = None\n",
    "    while results == None:\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        html = resp.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        results = soup.find('div', id='search-results-list')\n",
    "        if results == None:\n",
    "            print(f\"a retrying page {i}... url: {url}\")\n",
    "            time.sleep(2)\n",
    "    if resp.status_code == 200:\n",
    "        count = 0\n",
    "        for child in results:\n",
    "            check = str(child.attrs.items())\n",
    "            if \"css-1792eun\" in check or \"css1792eun\" in check:\n",
    "                count += 1\n",
    "                if \"Available Online\" not in child.text:\n",
    "                    title = child.text.split(\".\")[0]\n",
    "                    link = child.find(\"a\", attrs={\"role\": \"link\"})[\"href\"]\n",
    "                    url = \"https://www.nypl.org\" + link\n",
    "                    get_class(url, title, i)\n",
    "    else:\n",
    "        print(f\"Page {i} error: {resp.status_code}\")\n",
    "\n",
    "output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Web scrape box entry on catalog\n",
    "        \n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"}\n",
    "for i in range(1,8):\n",
    "    url = f\"https://www.nypl.org/research/research-catalog/bib/b16187984?item_page={i}\"\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    html = resp.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = soup.find(\"table\", attrs={\"id\": \"bib-item-table\"}).find(\"tbody\")\n",
    "    for child in results.children:\n",
    "        test = child.find(\"td\", attrs={\"data-th\": \"Call Number\"})\n",
    "        print(test.find(\"span\").text.rstrip())\n",
    "\n",
    "output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matches between boxes in catalog and white studios in catalog\n",
    "import csv\n",
    "\n",
    "catalog_file = open(\"catalog-scraping-final.csv\", \"r\")\n",
    "catalog = csv.reader(catalog_file, delimiter='\\t')\n",
    "\n",
    "box_file = open(\"anniemarie.txt\", \"r\")\n",
    "box = box_file.read().splitlines()\n",
    "\n",
    "for box_entry in box:\n",
    "    box_list = box_entry.split()\n",
    "    box_num = box_list[4][:-1]\n",
    "\n",
    "    for catalog_entry in catalog:\n",
    "        catalog_page_list = catalog_entry[2].split(\"p.\")\n",
    "        catalog_list = catalog_page_list[0].split()\n",
    "        if \"MWEZ\" in catalog_list:\n",
    "            catalog_num = catalog_list[3][:-1]\n",
    "            if catalog_num == box_num:\n",
    "                print(\"Catalog: \",end=\"\")\n",
    "                print(\" \".join(catalog_entry[1:]))\n",
    "                print(\" \".join(box_list[5:]),end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
